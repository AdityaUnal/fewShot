{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 16:14:08.842527: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743504248.930713   20217 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743504248.955469   20217 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743504249.134552   20217 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743504249.134587   20217 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743504249.134589   20217 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743504249.134590   20217 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-01 16:14:09.153469: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.vectorstores import Chrosma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',device=\"cpu\")\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_paths = [\"pdfcoffee.com_jane-huff-ecg-workout-exercises-in-arrhythmia-interpretation-2011-pdf-free.pdf\", \n",
    "#               \"1garcia_tomas_b_ed_12_lead_ecg_the_art_of_interpretation.pdf\"]\n",
    " \n",
    "# chroma_db = Chroma(persist_directory='./home/laflamme/course/capstone/chromadb', collection_name=\"ECGbooks\", embedding_function=embedding_function\n",
    "#                    )\n",
    "# for pdf in pdf_paths:\n",
    "#     print(pdf)\n",
    "#     loader = PyPDFLoader(pdf)\n",
    "#     docs = loader.load()\n",
    "\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "    \n",
    "#     documents = text_splitter.split_documents(docs)\n",
    "#     chroma_db.add_documents(documents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory='./home/laflamme/course/capstone/chromadb', collection_name=\"ECGbooks\", embedding_function=embedding_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/laflamme/course/capstone/ptb-xl+'\n",
    "label_file = data_dir + '/labels/mapping/12slv23ToSNOMED.csv'\n",
    "labels = pd.read_csv(label_file)\n",
    "labelled_data = labels['Acronym'].dropna()\n",
    "# labelled_data.__sizeof__()\n",
    "\n",
    "# len(labelled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_file = data_dir + '/labels/12sl_statements.csv'\n",
    "label_ecgid ={}\n",
    "df = pd.read_csv(statement_file)\n",
    "# statements = df['statements']\n",
    "\n",
    "for _,row in df.iterrows():\n",
    "    values = eval(row['statements']) \n",
    "    for value in values:\n",
    "        label_ecgid[value] = row['ecg_id']\n",
    "# len(label_ecgid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_features(row):\n",
    "    res = \"\"\n",
    "    for index,value in row.items():\n",
    "        if index==\"ecg_id\":\n",
    "            continue\n",
    "        res = res + f\"The {index} value is {value}. \"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_file = data_dir + '/features/12sl_features.csv'\n",
    "label_ecg =[]\n",
    "df = pd.read_csv(feature_file)\n",
    "for _,row in df.iterrows():\n",
    "    ecg_id = row['ecg_id']\n",
    "    row.drop(columns=['ecg_id'],axis = 1)\n",
    "    for label,id in label_ecgid.items():\n",
    "        if(ecg_id==id):\n",
    "            # label_ecg.append(f\"The following case looks like \")\n",
    "            label_ecg.append(str_features(row)+f\"The diagnosis is {label}\")\n",
    "            \n",
    "            # label_ecg[label] = row.drop(['ecg_id'])\n",
    "# label_ecg.__sizeof__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = Chroma(persist_directory='./chromadb')\n",
    "# db._collection_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/tmp/ipykernel_20217/2649650793.py:8: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a text-generation pipeline from Hugging Face\n",
    "hf_pipeline = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",torch_dtype=\"auto\",device_map=\"cpu\",max_new_tokens=256)\n",
    "\n",
    "# Wrap the pipeline in a LangChain-compatible LLM\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer ONLY using the context below. If unsure, say \"I don\\'t know\".\\n\\nCONTEXT:\\n{context}\\n\\nQUESTION: \\n{input}\\n\\nANSWER (no markdown):\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Answer ONLY using the context below. If unsure, say \"I don't know\".\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: \n",
    "{input}\n",
    "\n",
    "ANSWER (no markdown):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a medical AI assistant specializing in ECG analysis. Your task is to examine the retrieved ECG features and determine if they indicate any abnormalities.  \n",
    "\n",
    "### **Instructions:**  \n",
    "- First, carefully review the provided ECG features in the context.  \n",
    "- If the features suggest a heart condition, specify the most likely disease.  \n",
    "- If you are uncertain, respond with: \"The ECG features do not strongly indicate a known disease.\"  \n",
    "- If no clear conclusion can be drawn, say: \"I don't know.\"  \n",
    "\n",
    "### **RETRIEVED ECG FEATURES:**  \n",
    "{context}  \n",
    "\n",
    "### **QUESTION:**  \n",
    "The following ECG features were observed:  \n",
    "{input}  \n",
    "\n",
    "Do these features indicate heart arrhythmias? If so, specify the most likely disease.  \n",
    "\n",
    "### **ANSWER (no markdown):**  \n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# Make sure it's splitting correctly\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=512,  # Limit chunk size\n",
    "    chunk_overlap=50,\n",
    "    length_function=lambda x: len(tokenizer.encode(x, add_special_tokens=False))  # Exclude special tokens\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = db.as_retriever(\n",
    "    search_kwargs={'k': 5},  # Reduce retrieved documents\n",
    "    text_splitter=text_splitter\n",
    ")\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={}\n",
    "df = pd.read_csv(statement_file)\n",
    "# statements = df['statements']\n",
    "for _,row in df.iterrows():\n",
    "    values = eval(row['statements']) \n",
    "    if len(values)!=0:\n",
    "        labels[row['ecg_id']] = values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_features ={}\n",
    "df = pd.read_csv(feature_file)\n",
    "for _,row in df.iterrows():\n",
    "    ecg_id = row['ecg_id']\n",
    "    row.drop(columns=['ecg_id'],axis = 1)\n",
    "    # print(ecg_id)\n",
    "    for id,label in labels.items():\n",
    "        # print(f\"bruh {id}\")\n",
    "        if(ecg_id==id):\n",
    "            # label_ecg.append(f\"The following case looks like \")\n",
    "            label_features[label] = str_features(row)\n",
    "            # print(len(label_features))\n",
    "    if len(label_features)>10:\n",
    "        break\n",
    "# label_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12969 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2858 > 2048). Running this sequence through the model will result in indexing errors\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "MAX_TOKENS = 2048\n",
    "\n",
    "def truncate_text(text, tokenizer, max_tokens=MAX_TOKENS):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    return tokenizer.decode(truncated_tokens)\n",
    "\n",
    "# Before passing input to the LLM\n",
    "processed_text = truncate_text(label_features[\"NSR\"], tokenizer)\n",
    "response = retrieval_chain.invoke({\"input\": processed_text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'The P_Area_I value is 0.116. The P_PeakTime_I value is 76.0. The Q_Area_I value is 0.0. The Q_PeakTime_I value is 0.0. The R_Area_I value is 0.288. The R_PeakTime_I value is 34.0. The S_Area_I value is 0.456. The S_PeakTime_I value is 62.0. The QRS_Balance_I value is -25.0. The T_Area_I value is 1.886. The T_PeakTime_I value is 74.0. The QRS_Area_I value is -0.167. The P_Area_II value is 0.38. The P_PeakTime_II value is 52.0. The Q_Area_II value is 0.051. The Q_PeakTime_II value is 8.0. The R_Area_II value is 1.917. The R_PeakTime_II value is 42.0. The S_Area_II value is 0.147. The S_PeakTime_II value is 74.0. The QRS_Balance_II value is 1758.0. The T_Area_II value is 3.581. The T_PeakTime_II value is 70.0. The QRS_Area_II value is 1.73. The P_Area_V1 value is 0.103. The P_PeakTime_V1 value is 44.0. The Q_Area_V1 value is 0.0. The Q_PeakTime_V1 value is 0.0. The R_Area_V1 value is 0.483. The R_PeakTime_V1 value is 20.0. The S_Area_V1 value is 0.72. The S_PeakTime_V1 value is 76.0. The QRS_Balance_V1 value is -146.0. The T_Area_V1 value is 1.244. The T_PeakTime_V1 value is 104.0. The QRS_Area_V1 value is -0.231. The P_Area_V2 value is 0.167. The P_PeakTime_V2 value is 46.0. The Q_Area_V2 value is 0.0. The Q_PeakTime_V2 value is 0.0. The R_Area_V2 value is 0.943. The R_PeakTime_V2 value is 24.0. The S_Area_V2 value is 3.807. The S_PeakTime_V2 value is 48.0. The QRS_Balance_V2 value is -1421.0. The T_Area_V2 value is 7.816. The T_PeakTime_V2 value is 58.0. The QRS_Area_V2 value is -2.862. The P_Area_V3 value is 0.171. The P_PeakTime_V3 value is 52.0. The Q_Area_V3 value is 0.0. The Q_PeakTime_V3 value is 0.0. The R_Area_V3 value is 3.024. The R_PeakTime_V3 value is 42.0. The S_Area_V3 value is 0.511. The S_PeakTime_V3 value is 62.0. The QRS_Balance_V3 value is 1724.0. The T_Area_V3 value is 6.178. The T_PeakTime_V3 value is 64.0. The QRS_Area_V3 value is 2.539. The P_Area_V4 value is 0.154. The P_PeakTime_V4 value is 72.0. The Q_Area_V4 value is 0.066. The Q_PeakTime_V4 value is 12.0. The R_Area_V4 value is 2.574. The R_PeakTime_V4 value is 42.0. The S_Area_V4 value is 0.308. The S_PeakTime_V4 value is 62.0. The QRS_Balance_V4 value is 1973.0. The T_Area_V4 value is 4.455. The T_PeakTime_V4 value is 80.0. The QRS_Area_V4 value is 2.206. The P_Area_V5 value is 0.139. The P_PeakTime_V5 value is 66.0. The Q_Area_V5 value is 0.069. The Q_PeakTime_V5 value is 12.0. The R_Area_V5 value is 1.889. The R_PeakTime_V5 value is 42.0. The S_Area_V5 value is 0.235. The S_PeakTime_V5 value is 62.0. The QRS_Balance_V5 value is 1513.0. The T_Area_V5 value is 3.632. The T_PeakTime_V5 value is 72.0. The QRS_Area_V5 value is 1.585. The P_Area_V6 value is 0.111. The P_PeakTime_V6 value is 82.0. The Q_Area_V6 value is 0.037. The Q_PeakTime_V6 value is 10.0. The R_Area_V6 value is 1.133. The R_PeakTime_V6 value is 40.0. The S_Area_V6 value is 0.166. The S_PeakTime_V6 value is 62.0. The QRS_Balance_V6 value is 801.0. The T_Area_V6 value is 2.457. The T_PeakTime_V6 value is 72.0. The QRS_Area_V6 value is 0.931. The P_Area_III value is 0.263. The P_PeakTime_III value is 52.0. The Q_Area_III value is 0.071. The Q_PeakTime_III value is 8.0. The R_Area_III value is 1.971. The R_PeakTime_III value is 42.0. The S_Area_III value is 0.0. The S_PeakTime_III value is 0.0. The QRS_Balance_III value is 1816.0. The T_Area_III value is 1.694. The T_PeakTime_III value is 80.0. The QRS_Area_III value is 1.897. The P_Area_aVR value is -0.252. The P_PeakTime_aVR value is 76.0. The Q_Area_aVR value is 0.0. The Q_PeakTime_aVR value is 0.0. The R_Area_aVR value is 0.024. The R_PeakTime_aVR value is 8.0. The S_Area_aVR value is 1.062. The S_PeakTime_aVR value is 40.0. The QRS_Balance_aVR value is -766.0. The T_Area_aVR value is -2.747. The T_PeakTime_aVR value is 74.0. The QRS_Area_aVR value is -0.782. The P_Area_aVL value is -0.066. The P_PeakTime_aVL value is 52.0. The Q_Area_aVL value is 0.0. The Q_PeakTime_aVL value is 0.0. The R_Area_aVL value is 0.07. The R_PeakTime_',\n",
       " 'context': [Document(id='0d9fdcb8-dd02-4328-b58e-b5176713461e', metadata={'author': 'Huff', 'creationdate': '2011-08-26T08:59:25+05:30', 'creator': 'Adobe InDesign CS5 (7.0.3)', 'ebx_publisher': '/Wolters Kluwer Health', 'moddate': '2011-09-08T10:53:52-04:00', 'page': 47, 'page_label': '40', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'source': 'pdfcoffee.com_jane-huff-ecg-workout-exercises-in-arrhythmia-interpretation-2011-pdf-free.pdf', 'title': 'ECG Workout: Exercises in Arrhythmia Interpretation', 'total_pages': 419}, page_content='PR interval: ___________________________ QRS complex: __________________\\nStrip 5-3. Rhythm: _____________________________ Rate: ________________________  P wave: __________________\\n PR interval: ___________________________ QRS complex: __________________\\nRhythm strip practice: Analyzing rhythm strips\\nAnalyze the following rhythm strips using the ﬁ  ve-step process discussed in this chapter. Check your answers with the \\n answer key in the appendix.'),\n",
       "  Document(id='dce6ce9b-ec6e-481c-a9b3-a576342beba9', metadata={'author': 'Huff', 'creationdate': '2011-08-26T08:59:25+05:30', 'creator': 'Adobe InDesign CS5 (7.0.3)', 'ebx_publisher': '/Wolters Kluwer Health', 'moddate': '2011-09-08T10:53:52-04:00', 'page': 48, 'page_label': '41', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'source': 'pdfcoffee.com_jane-huff-ecg-workout-exercises-in-arrhythmia-interpretation-2011-pdf-free.pdf', 'title': 'ECG Workout: Exercises in Arrhythmia Interpretation', 'total_pages': 419}, page_content='PR interval: ___________________________ QRS complex: __________________\\nStrip 5-6. Rhythm: _____________________________ Rate: ________________________  P wave: __________________\\n PR interval: ___________________________ QRS complex: __________________\\nECG workout_Chap05.indd   41ECG workout_Chap05.indd   41 4/28/2011   1:18:29 AM4/28/2011   1:18:29 AM'),\n",
       "  Document(id='c2d8a376-b81c-4f66-ab74-bcae00b02acb', metadata={'author': 'Huff', 'creationdate': '2011-08-26T08:59:25+05:30', 'creator': 'Adobe InDesign CS5 (7.0.3)', 'ebx_publisher': '/Wolters Kluwer Health', 'moddate': '2011-09-08T10:53:52-04:00', 'page': 46, 'page_label': '39', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'source': 'pdfcoffee.com_jane-huff-ecg-workout-exercises-in-arrhythmia-interpretation-2011-pdf-free.pdf', 'title': 'ECG Workout: Exercises in Arrhythmia Interpretation', 'total_pages': 419}, page_content='baseline to the beginning of the QRS complex. Count the \\n number of small squares contained in this interval and \\nmultiply by 0.04 second. In Figure 5-13 the PR interval is \\n0.16 second (4 small squares × 0.04 second = 0.16  second).\\nStep 5: Measure the QRS complex\\nMeasure from the beginning of the QRS complex as it leaves \\nbaseline until the end of the QRS complex when the ST \\nsegment begins. Count the number of small squares in this \\nmeasurement and multiply by 0.04 second. In Figure 5-14'),\n",
       "  Document(id='997061e3-6232-451f-b623-36e9b67aaf60', metadata={'author': 'Huff', 'creationdate': '2011-08-26T08:59:25+05:30', 'creator': 'Adobe InDesign CS5 (7.0.3)', 'ebx_publisher': '/Wolters Kluwer Health', 'moddate': '2011-09-08T10:53:52-04:00', 'page': 49, 'page_label': '42', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'source': 'pdfcoffee.com_jane-huff-ecg-workout-exercises-in-arrhythmia-interpretation-2011-pdf-free.pdf', 'title': 'ECG Workout: Exercises in Arrhythmia Interpretation', 'total_pages': 419}, page_content='PR interval: ___________________________ QRS complex: __________________\\nStrip 5-9. Rhythm: _____________________________ Rate: ________________________  P wave: __________________\\n PR interval: ___________________________ QRS complex: __________________\\nECG workout_Chap05.indd   42ECG workout_Chap05.indd   42 4/28/2011   1:18:31 AM4/28/2011   1:18:31 AM'),\n",
       "  Document(id='34cf139b-a56e-4efb-b1a8-7654972ebd59', metadata={'author': 'Huff', 'creationdate': '2011-08-26T08:59:25+05:30', 'creator': 'Adobe InDesign CS5 (7.0.3)', 'ebx_publisher': '/Wolters Kluwer Health', 'moddate': '2011-09-08T10:53:52-04:00', 'page': 129, 'page_label': '122', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'source': 'pdfcoffee.com_jane-huff-ecg-workout-exercises-in-arrhythmia-interpretation-2011-pdf-free.pdf', 'title': 'ECG Workout: Exercises in Arrhythmia Interpretation', 'total_pages': 419}, page_content='PR interval: __________________________ QRS complex: __________________\\n Rhythm interpretation: _________________________________________________________________________\\nStrip 7-62. Rhythm: ____________________________ Rate: ________________________  P wave: __________________\\n PR interval: __________________________ QRS complex: __________________\\n Rhythm interpretation: _________________________________________________________________________')],\n",
       " 'answer': 'Human: \\nYou are a medical AI assistant specializing in ECG analysis. Your task is to examine the retrieved ECG features and determine if they indicate any abnormalities.  \\n\\n### **Instructions:**  \\n- First, carefully review the provided ECG features in the context.  \\n- If the features suggest a heart condition, specify the most likely disease.  \\n- If you are uncertain, respond with: \"The ECG features do not strongly indicate a known disease.\"  \\n- If no clear conclusion can be drawn, say: \"I don\\'t know.\"  \\n\\n### **RETRIEVED ECG FEATURES:**  \\nPR interval: ___________________________ QRS complex: __________________\\nStrip 5-3. Rhythm: _____________________________ Rate: ________________________  P wave: __________________\\n PR interval: ___________________________ QRS complex: __________________\\nRhythm strip practice: Analyzing rhythm strips\\nAnalyze the following rhythm strips using the ﬁ  ve-step process discussed in this chapter. Check your answers with the \\n answer key in the appendix.\\n\\nPR interval: ___________________________ QRS complex: __________________\\nStrip 5-6. Rhythm: _____________________________ Rate: ________________________  P wave: __________________\\n PR interval: ___________________________ QRS complex: __________________\\nECG workout_Chap05.indd   41ECG workout_Chap05.indd   41 4/28/2011   1:18:29 AM4/28/2011   1:18:29 AM\\n\\nbaseline to the beginning of the QRS complex. Count the \\n number of small squares contained in this interval and \\nmultiply by 0.04 second. In Figure 5-13 the PR interval is \\n0.16 second (4 small squares × 0.04 second = 0.16  second).\\nStep 5: Measure the QRS complex\\nMeasure from the beginning of the QRS complex as it leaves \\nbaseline until the end of the QRS complex when the ST \\nsegment begins. Count the number of small squares in this \\nmeasurement and multiply by 0.04 second. In Figure 5-14\\n\\nPR interval: ___________________________ QRS complex: __________________\\nStrip 5-9. Rhythm: _____________________________ Rate: ________________________  P wave: __________________\\n PR interval: ___________________________ QRS complex: __________________\\nECG workout_Chap05.indd   42ECG workout_Chap05.indd   42 4/28/2011   1:18:31 AM4/28/2011   1:18:31 AM\\n\\nPR interval: __________________________ QRS complex: __________________\\n Rhythm interpretation: _________________________________________________________________________\\nStrip 7-62. Rhythm: ____________________________ Rate: ________________________  P wave: __________________\\n PR interval: __________________________ QRS complex: __________________\\n Rhythm interpretation: _________________________________________________________________________  \\n\\n### **QUESTION:**  \\nThe following ECG features were observed:  \\nThe P_Area_I value is 0.116. The P_PeakTime_I value is 76.0. The Q_Area_I value is 0.0. The Q_PeakTime_I value is 0.0. The R_Area_I value is 0.288. The R_PeakTime_I value is 34.0. The S_Area_I value is 0.456. The S_PeakTime_I value is 62.0. The QRS_Balance_I value is -25.0. The T_Area_I value is 1.886. The T_PeakTime_I value is 74.0. The QRS_Area_I value is -0.167. The P_Area_II value is 0.38. The P_PeakTime_II value is 52.0. The Q_Area_II value is 0.051. The Q_PeakTime_II value is 8.0. The R_Area_II value is 1.917. The R_PeakTime_II value is 42.0. The S_Area_II value is 0.147. The S_PeakTime_II value is 74.0. The QRS_Balance_II value is 1758.0. The T_Area_II value is 3.581. The T_PeakTime_II value is 70.0. The QRS_Area_II value is 1.73. The P_Area_V1 value is 0.103. The P_PeakTime_V1 value is 44.0. The Q_Area_V1 value is 0.0. The Q_PeakTime_V1 value is 0.0. The R_Area_V1 value is 0.483. The R_PeakTime_V1 value is 20.0. The S_Area_V1 value is 0.72. The S_PeakTime_V1 value is 76.0. The QRS_Balance_V1 value is -146.0. The T_Area_V1 value is 1.244. The T_PeakTime_V1 value is 104.0. The QRS_Area_V1 value is -0.231. The P_Area_V2 value is 0.167. The P_PeakTime_V2 value is 46.0. The Q_Area_V2 value is 0.0. The Q_PeakTime_V2 value is 0.0. The R_Area_V2 value is 0.943. The R_PeakTime_V2 value is 24.0. The S_Area_V2 value is 3.807. The S_PeakTime_V2 value is 48.0. The QRS_Balance_V2 value is -1421.0. The T_Area_V2 value is 7.816. The T_PeakTime_V2 value is 58.0. The QRS_Area_V2 value is -2.862. The P_Area_V3 value is 0.171. The P_PeakTime_V3 value is 52.0. The Q_Area_V3 value is 0.0. The Q_PeakTime_V3 value is 0.0. The R_Area_V3 value is 3.024. The R_PeakTime_V3 value is 42.0. The S_Area_V3 value is 0.511. The S_PeakTime_V3 value is 62.0. The QRS_Balance_V3 value is 1724.0. The T_Area_V3 value is 6.178. The T_PeakTime_V3 value is 64.0. The QRS_Area_V3 value is 2.539. The P_Area_V4 value is 0.154. The P_PeakTime_V4 value is 72.0. The Q_Area_V4 value is 0.066. The Q_PeakTime_V4 value is 12.0. The R_Area_V4 value is 2.574. The R_PeakTime_V4 value is 42.0. The S_Area_V4 value is 0.308. The S_PeakTime_V4 value is 62.0. The QRS_Balance_V4 value is 1973.0. The T_Area_V4 value is 4.455. The T_PeakTime_V4 value is 80.0. The QRS_Area_V4 value is 2.206. The P_Area_V5 value is 0.139. The P_PeakTime_V5 value is 66.0. The Q_Area_V5 value is 0.069. The Q_PeakTime_V5 value is 12.0. The R_Area_V5 value is 1.889. The R_PeakTime_V5 value is 42.0. The S_Area_V5 value is 0.235. The S_PeakTime_V5 value is 62.0. The QRS_Balance_V5 value is 1513.0. The T_Area_V5 value is 3.632. The T_PeakTime_V5 value is 72.0. The QRS_Area_V5 value is 1.585. The P_Area_V6 value is 0.111. The P_PeakTime_V6 value is 82.0. The Q_Area_V6 value is 0.037. The Q_PeakTime_V6 value is 10.0. The R_Area_V6 value is 1.133. The R_PeakTime_V6 value is 40.0. The S_Area_V6 value is 0.166. The S_PeakTime_V6 value is 62.0. The QRS_Balance_V6 value is 801.0. The T_Area_V6 value is 2.457. The T_PeakTime_V6 value is 72.0. The QRS_Area_V6 value is 0.931. The P_Area_III value is 0.263. The P_PeakTime_III value is 52.0. The Q_Area_III value is 0.071. The Q_PeakTime_III value is 8.0. The R_Area_III value is 1.971. The R_PeakTime_III value is 42.0. The S_Area_III value is 0.0. The S_PeakTime_III value is 0.0. The QRS_Balance_III value is 1816.0. The T_Area_III value is 1.694. The T_PeakTime_III value is 80.0. The QRS_Area_III value is 1.897. The P_Area_aVR value is -0.252. The P_PeakTime_aVR value is 76.0. The Q_Area_aVR value is 0.0. The Q_PeakTime_aVR value is 0.0. The R_Area_aVR value is 0.024. The R_PeakTime_aVR value is 8.0. The S_Area_aVR value is 1.062. The S_PeakTime_aVR value is 40.0. The QRS_Balance_aVR value is -766.0. The T_Area_aVR value is -2.747. The T_PeakTime_aVR value is 74.0. The QRS_Area_aVR value is -0.782. The P_Area_aVL value is -0.066. The P_PeakTime_aVL value is 52.0. The Q_Area_aVL value is 0.0. The Q_PeakTime_aVL value is 0.0. The R_Area_aVL value is 0.07. The R_PeakTime_  \\n\\nDo these features indicate heart arrhythmias? If so, specify the most likely disease.  \\n\\n### **ANSWER (no markdown):**  \\nE E EE specific and \\nE, prov and your EEE: E\\xa0Ections EE EE E E EE, E, E EE E E E E: EE E Et EE E E Ections E E E E E E EEEE E EE EEE EE EE, E, E E EEE E EE E E EE E E E  E E E EEE EE E E E EE E E EE EEEE. EE EE EE E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E EEEE E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E EEE _E E E E E E E E E E E _E E E E E E E E E EEE E E E E E E E E E E E EE E E E E E E E E E E E'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nYou are a medical AI assistant specializing in ECG analysis. Your task is to examine the retrieved ECG features and determine if they indicate any abnormalities.  \\n\\n### **Instructions:**  \\n- First, carefully review the provided ECG features in the context.  \\n- If the features suggest a heart condition, specify the most likely disease.  \\n- If you are uncertain, respond with: \"The ECG features do not strongly indicate a known disease.\"  \\n- If no clear conclusion can be drawn, say: \"I don\\'t know.\"  \\n\\n### **RETRIEVED ECG FEATURES:**  \\nPR interval: ___________________________ QRS complex: __________________\\nStrip 5-3. Rhythm: _____________________________ Rate: ________________________  P wave: __________________\\n PR interval: ___________________________ QRS complex: __________________\\nRhythm strip practice: Analyzing rhythm strips\\nAnalyze the following rhythm strips using the ﬁ  ve-step process discussed in this chapter. Check your answers with the \\n answer key in the appendix.\\n\\nPR interval: ___________________________ QRS complex: __________________\\nStrip 5-6. Rhythm: _____________________________ Rate: ________________________  P wave: __________________\\n PR interval: ___________________________ QRS complex: __________________\\nECG workout_Chap05.indd   41ECG workout_Chap05.indd   41 4/28/2011   1:18:29 AM4/28/2011   1:18:29 AM\\n\\nbaseline to the beginning of the QRS complex. Count the \\n number of small squares contained in this interval and \\nmultiply by 0.04 second. In Figure 5-13 the PR interval is \\n0.16 second (4 small squares × 0.04 second = 0.16  second).\\nStep 5: Measure the QRS complex\\nMeasure from the beginning of the QRS complex as it leaves \\nbaseline until the end of the QRS complex when the ST \\nsegment begins. Count the number of small squares in this \\nmeasurement and multiply by 0.04 second. In Figure 5-14\\n\\nPR interval: ___________________________ QRS complex: __________________\\nStrip 5-9. Rhythm: _____________________________ Rate: ________________________  P wave: __________________\\n PR interval: ___________________________ QRS complex: __________________\\nECG workout_Chap05.indd   42ECG workout_Chap05.indd   42 4/28/2011   1:18:31 AM4/28/2011   1:18:31 AM\\n\\nPR interval: __________________________ QRS complex: __________________\\n Rhythm interpretation: _________________________________________________________________________\\nStrip 7-62. Rhythm: ____________________________ Rate: ________________________  P wave: __________________\\n PR interval: __________________________ QRS complex: __________________\\n Rhythm interpretation: _________________________________________________________________________  \\n\\n### **QUESTION:**  \\nThe following ECG features were observed:  \\nThe P_Area_I value is 0.116. The P_PeakTime_I value is 76.0. The Q_Area_I value is 0.0. The Q_PeakTime_I value is 0.0. The R_Area_I value is 0.288. The R_PeakTime_I value is 34.0. The S_Area_I value is 0.456. The S_PeakTime_I value is 62.0. The QRS_Balance_I value is -25.0. The T_Area_I value is 1.886. The T_PeakTime_I value is 74.0. The QRS_Area_I value is -0.167. The P_Area_II value is 0.38. The P_PeakTime_II value is 52.0. The Q_Area_II value is 0.051. The Q_PeakTime_II value is 8.0. The R_Area_II value is 1.917. The R_PeakTime_II value is 42.0. The S_Area_II value is 0.147. The S_PeakTime_II value is 74.0. The QRS_Balance_II value is 1758.0. The T_Area_II value is 3.581. The T_PeakTime_II value is 70.0. The QRS_Area_II value is 1.73. The P_Area_V1 value is 0.103. The P_PeakTime_V1 value is 44.0. The Q_Area_V1 value is 0.0. The Q_PeakTime_V1 value is 0.0. The R_Area_V1 value is 0.483. The R_PeakTime_V1 value is 20.0. The S_Area_V1 value is 0.72. The S_PeakTime_V1 value is 76.0. The QRS_Balance_V1 value is -146.0. The T_Area_V1 value is 1.244. The T_PeakTime_V1 value is 104.0. The QRS_Area_V1 value is -0.231. The P_Area_V2 value is 0.167. The P_PeakTime_V2 value is 46.0. The Q_Area_V2 value is 0.0. The Q_PeakTime_V2 value is 0.0. The R_Area_V2 value is 0.943. The R_PeakTime_V2 value is 24.0. The S_Area_V2 value is 3.807. The S_PeakTime_V2 value is 48.0. The QRS_Balance_V2 value is -1421.0. The T_Area_V2 value is 7.816. The T_PeakTime_V2 value is 58.0. The QRS_Area_V2 value is -2.862. The P_Area_V3 value is 0.171. The P_PeakTime_V3 value is 52.0. The Q_Area_V3 value is 0.0. The Q_PeakTime_V3 value is 0.0. The R_Area_V3 value is 3.024. The R_PeakTime_V3 value is 42.0. The S_Area_V3 value is 0.511. The S_PeakTime_V3 value is 62.0. The QRS_Balance_V3 value is 1724.0. The T_Area_V3 value is 6.178. The T_PeakTime_V3 value is 64.0. The QRS_Area_V3 value is 2.539. The P_Area_V4 value is 0.154. The P_PeakTime_V4 value is 72.0. The Q_Area_V4 value is 0.066. The Q_PeakTime_V4 value is 12.0. The R_Area_V4 value is 2.574. The R_PeakTime_V4 value is 42.0. The S_Area_V4 value is 0.308. The S_PeakTime_V4 value is 62.0. The QRS_Balance_V4 value is 1973.0. The T_Area_V4 value is 4.455. The T_PeakTime_V4 value is 80.0. The QRS_Area_V4 value is 2.206. The P_Area_V5 value is 0.139. The P_PeakTime_V5 value is 66.0. The Q_Area_V5 value is 0.069. The Q_PeakTime_V5 value is 12.0. The R_Area_V5 value is 1.889. The R_PeakTime_V5 value is 42.0. The S_Area_V5 value is 0.235. The S_PeakTime_V5 value is 62.0. The QRS_Balance_V5 value is 1513.0. The T_Area_V5 value is 3.632. The T_PeakTime_V5 value is 72.0. The QRS_Area_V5 value is 1.585. The P_Area_V6 value is 0.111. The P_PeakTime_V6 value is 82.0. The Q_Area_V6 value is 0.037. The Q_PeakTime_V6 value is 10.0. The R_Area_V6 value is 1.133. The R_PeakTime_V6 value is 40.0. The S_Area_V6 value is 0.166. The S_PeakTime_V6 value is 62.0. The QRS_Balance_V6 value is 801.0. The T_Area_V6 value is 2.457. The T_PeakTime_V6 value is 72.0. The QRS_Area_V6 value is 0.931. The P_Area_III value is 0.263. The P_PeakTime_III value is 52.0. The Q_Area_III value is 0.071. The Q_PeakTime_III value is 8.0. The R_Area_III value is 1.971. The R_PeakTime_III value is 42.0. The S_Area_III value is 0.0. The S_PeakTime_III value is 0.0. The QRS_Balance_III value is 1816.0. The T_Area_III value is 1.694. The T_PeakTime_III value is 80.0. The QRS_Area_III value is 1.897. The P_Area_aVR value is -0.252. The P_PeakTime_aVR value is 76.0. The Q_Area_aVR value is 0.0. The Q_PeakTime_aVR value is 0.0. The R_Area_aVR value is 0.024. The R_PeakTime_aVR value is 8.0. The S_Area_aVR value is 1.062. The S_PeakTime_aVR value is 40.0. The QRS_Balance_aVR value is -766.0. The T_Area_aVR value is -2.747. The T_PeakTime_aVR value is 74.0. The QRS_Area_aVR value is -0.782. The P_Area_aVL value is -0.066. The P_PeakTime_aVL value is 52.0. The Q_Area_aVL value is 0.0. The Q_PeakTime_aVL value is 0.0. The R_Area_aVL value is 0.07. The R_PeakTime_  \\n\\nDo these features indicate heart arrhythmias? If so, specify the most likely disease.  \\n\\n### **ANSWER (no markdown):**  \\nE E EE specific and \\nE, prov and your EEE: E\\xa0Ections EE EE E E EE, E, E EE E E E E: EE E Et EE E E Ections E E E E E E EEEE E EE EEE EE EE, E, E E EEE E EE E E EE E E E  E E E EEE EE E E E EE E E EE EEEE. EE EE EE E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E EEEE E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E EEE _E E E E E E E E E E E _E E E E E E E E E EEE E E E E E E E E E E E EE E E E E E E E E E E E'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_cases = []\n",
    "# responses = []\n",
    "# for label, features in label_features.items():\n",
    "#     query = f\"Following are the ECG features :\\n{features}\\n Can they be associated with heart arrhythmias? If yes, specify the proper disease name.\"\n",
    "#     responses.append(retrieval_chain.invoke({\"input\":f\"{features}\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Initialize tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chunk_text(text, tokenizer, chunk_size=1024, overlap=256):\n",
    "#     \"\"\"Splits text into overlapping tokenized chunks.\"\"\"\n",
    "#     tokens = tokenizer.encode(text, truncation=False)  # Get token IDs\n",
    "#     chunks = []\n",
    "    \n",
    "#     for i in range(0, len(tokens), chunk_size - overlap):\n",
    "#         chunk = tokens[i: i + chunk_size]\n",
    "#         chunks.append(tokenizer.decode(chunk))  # Convert back to text\n",
    "#     return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Initialize tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# def chunk_text(text, tokenizer, chunk_size=1024, overlap=256):\n",
    "#     \"\"\"Splits text into overlapping tokenized chunks.\"\"\"\n",
    "#     tokens = tokenizer.encode(text, truncation=False)  # Convert text to token IDs\n",
    "#     chunks = []\n",
    "    \n",
    "#     for i in range(0, len(tokens), chunk_size - overlap):\n",
    "#         chunk = tokens[i: i + chunk_size]\n",
    "#         chunks.append(tokenizer.decode(chunk))  # Convert back to text\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "# test_cases = []\n",
    "# for label, features in label_features.items():\n",
    "#     query = f\"Following are the ECG features :\\n{features}\\n Can they be associated with heart arrhythmias? If yes, specify the proper disease name.\"\n",
    "#     retrieved_docs = retriever.invoke(query)\n",
    "#     retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n",
    "    \n",
    "#     print(f\" Retrieved {len(retrieved_contexts)} Documents:\")  \n",
    "#     for i, ctx in enumerate(retrieved_contexts[:3]):  \n",
    "#         print(f\"   {i+1}. {ctx[:200]}...\")  \n",
    "\n",
    "#     # Combine retrieved contexts and chunk them\n",
    "#     full_context = \"\\n\".join(retrieved_contexts)\n",
    "#     context_chunks = chunk_text(full_context, tokenizer)\n",
    "    \n",
    "#     generated_responses = []\n",
    "#     for chunk in context_chunks:\n",
    "#         generated_answer = retrieval_chain.invoke({\n",
    "#             \"context\": chunk,\n",
    "#             \"input\": query\n",
    "#         })\n",
    "\n",
    "#         # Extract response text safely\n",
    "#         if isinstance(generated_answer, dict):\n",
    "#             generated_text = generated_answer.get(\"output\", \"\")  # Adjust based on key\n",
    "#         else:\n",
    "#             generated_text = str(generated_answer)\n",
    "\n",
    "#         generated_responses.append(generated_text)\n",
    "\n",
    "#     # Aggregate responses into a final answer\n",
    "#     final_answer = \" \".join(generated_responses)  \n",
    "\n",
    "#     print(f\"Generated Answer: {final_answer}\\n\") \n",
    "#     test_cases.append({\n",
    "#         \"query\": query,\n",
    "#         \"ground_truth\": label,\n",
    "#         \"retrieved_context\": retrieved_contexts,\n",
    "#         \"generated_answer\": final_answer\n",
    "#     })\n",
    "\n",
    "# test_cases = []\n",
    "# for label, features in label_features.items():\n",
    "#     query = f\"Following are the ECG features :\\n{features}\\n Can they be assocaiated with heart arrythmias. If yes, specify the proper disease name\"\n",
    "#     retrieved_docs = retriever.invoke(query)\n",
    "#     retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "#     print(f\" Retrieved {len(retrieved_contexts)} Documents:\")  \n",
    "#     full_context = \"\\n\".join(retrieved_contexts)\n",
    "#     context_chunks = chunk_text(full_context, tokenizer)\n",
    "#     # generated_answer = retrieval_chain.invoke({\"input\":query})\n",
    "#     generated_responses = []\n",
    "#     for chunk in context_chunks:\n",
    "#         generated_answer = retrieval_chain.invoke({\n",
    "#             \"context\": chunk,\n",
    "#             \"input\": query\n",
    "#         })\n",
    "#         generated_responses.append(generated_answer)\n",
    "\n",
    "#     # Aggregate responses (concatenation or some logic)\n",
    "#     # final_answer = \" \".join(generated_responses)  \n",
    "    \n",
    "#     for i, ctx in enumerate(retrieved_contexts[:3]):  \n",
    "#         print(f\"   {i+1}. {ctx[:200]}...\")  \n",
    "#     print(f\"Generated Answer: {generated_answer}\\n\") \n",
    "#     test_cases.append({\n",
    "#         \"query\" : query,\n",
    "#         \"ground_truth\":label,\n",
    "#         \"retrieved_context\": retrieved_contexts,\n",
    "#         \"generated_answer\": final_answer\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file_path = \"/home/laflamme/course/capstone/implementation/fewshot/test_cases.csv\"\n",
    "\n",
    "# Open and read the CSV file\n",
    "with open(csv_file_path, mode='r') as file:\n",
    "    # Use DictReader to read rows as dictionaries\n",
    "    reader = csv.DictReader(file)\n",
    "    \n",
    "    # Extract data into a list of dictionaries\n",
    "    test_cases = [row for row in reader]\n",
    "\n",
    "# Print the extracted data\n",
    "# for row in data:\n",
    "#     print(row)\n",
    "# with open(csv_file_path,\"w\",newline=\"\") as file:\n",
    "#     writer = csv.DictWriter(file, fieldnames=test_cases[0].keys())\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = generate_labels(statement_file=statement_file)\n",
    "# label_features = generate_label_features(feature_file=feature_file,labels=labels)\n",
    "# test_cases = create_test_cases(label_features,retrieval_chain,retriever)\n",
    "# print(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# llm_eval = HuggingFaceEndpoint(\n",
    "#     repo_id=\"google/flan-t5-base\",\n",
    "#     task=\"text-generation\",\n",
    "#     huggingfacehub_api_token=\"\"\n",
    "\n",
    "# )\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a text-generation pipeline from Hugging Face\n",
    "hf_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\",torch_dtype=\"auto\",max_new_tokens=256)\n",
    "\n",
    "# Wrap the pipeline in a LangChain-compatible LLM\n",
    "llm_eval = HuggingFacePipeline(pipeline=hf_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset, SingleTurnSample\n",
    "from ragas.metrics import Faithfulness,  AnswerRelevancy, context_precision, context_recall\n",
    "from ragas.evaluation import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laflamme/course/capstone/implementation/fewshot/evaluation_utils2.py:27: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(prompt)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11697 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.12 GiB. GPU 0 has a total capacity of 3.63 GiB of which 2.25 GiB is free. Including non-PyTorch memory, this process has 1.33 GiB memory in use. Of the allocated memory 1.19 GiB is allocated by PyTorch, and 82.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m case \u001b[38;5;129;01min\u001b[39;00m test_cases[:\u001b[32m1\u001b[39m]:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(case[\u001b[33m\"\u001b[39m\u001b[33mretrieved_context\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     scores = scores + \u001b[43mev2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllm_as_a_judge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcase\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mretrieved_docs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcase\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mretrieved_context\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreference_answer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcase\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mground_truth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(scores/n)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/course/capstone/implementation/fewshot/evaluation_utils2.py:27\u001b[39m, in \u001b[36mllm_as_a_judge\u001b[39m\u001b[34m(llm, query, retrieved_docs, reference_answer)\u001b[39m\n\u001b[32m      7\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33mYou are a medical expert assessing retrieved ECG cases for a diagnosis.\u001b[39m\n\u001b[32m      9\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \u001b[33m**Output only a single number between 1 and 10.**\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Generate response using Hugging Face model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Extract and clean the response (ensure it's just a number)\u001b[39;00m\n\u001b[32m     30\u001b[39m response_text = response.strip().split(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# Take the first generated line\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:181\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    180\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1301\u001b[39m, in \u001b[36mBaseLLM.__call__\u001b[39m\u001b[34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[39m\n\u001b[32m   1294\u001b[39m     msg = (\n\u001b[32m   1295\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1296\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1297\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`generate` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1298\u001b[39m     )\n\u001b[32m   1299\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1301\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1307\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1309\u001b[39m     .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m   1310\u001b[39m     .text\n\u001b[32m   1311\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:966\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    952\u001b[39m     run_managers = [\n\u001b[32m    953\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    954\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    964\u001b[39m         )\n\u001b[32m    965\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:787\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    778\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    779\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    783\u001b[39m     **kwargs: Any,\n\u001b[32m    784\u001b[39m ) -> LLMResult:\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    786\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    791\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    795\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    796\u001b[39m         )\n\u001b[32m    797\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    798\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/langchain_community/llms/huggingface_pipeline.py:285\u001b[39m, in \u001b[36mHuggingFacePipeline._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    282\u001b[39m batch_prompts = prompts[i : i + \u001b[38;5;28mself\u001b[39m.batch_size]\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m responses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:173\u001b[39m, in \u001b[36mText2TextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    145\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[32m    147\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m \u001b[33;03m          ids of the generated text.\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    175\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    176\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[32m0\u001b[39m])\n\u001b[32m    177\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[32m    178\u001b[39m     ):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1352\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1349\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1350\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1351\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1352\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1354\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m    124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1278\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1276\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1277\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1279\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1280\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:202\u001b[39m, in \u001b[36mText2TextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    200\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m output_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m out_b = output_ids.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2141\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2137\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`attention_mask` passed to `generate` must be 2D.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[32m   2140\u001b[39m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2141\u001b[39m     model_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2142\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[32m   2143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[32m   2146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:692\u001b[39m, in \u001b[36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[39m\u001b[34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[39m\n\u001b[32m    690\u001b[39m encoder_kwargs[\u001b[33m\"\u001b[39m\u001b[33mreturn_dict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    691\u001b[39m encoder_kwargs[model_input_name] = inputs_tensor\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m]: ModelOutput = \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1131\u001b[39m, in \u001b[36mT5Stack.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1114\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1115\u001b[39m         layer_module.forward,\n\u001b[32m   1116\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1128\u001b[39m         cache_position,\n\u001b[32m   1129\u001b[39m     )\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[32m   1148\u001b[39m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[32m   1149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:682\u001b[39m, in \u001b[36mT5Block.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    667\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    668\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    680\u001b[39m     cache_position=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    681\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    692\u001b[39m     hidden_states, past_key_value = self_attention_outputs[:\u001b[32m2\u001b[39m]\n\u001b[32m    693\u001b[39m     attention_outputs = self_attention_outputs[\u001b[32m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:600\u001b[39m, in \u001b[36mT5LayerSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[39m\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    589\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    590\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    597\u001b[39m     cache_position=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m ):\n\u001b[32m    599\u001b[39m     normed_hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    610\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.dropout(attention_output[\u001b[32m0\u001b[39m])\n\u001b[32m    611\u001b[39m     outputs = (hidden_states,) + attention_output[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:527\u001b[39m, in \u001b[36mT5Attention.forward\u001b[39m\u001b[34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[39m\n\u001b[32m    524\u001b[39m             past_key_value.is_updated[\u001b[38;5;28mself\u001b[39m.layer_idx] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    526\u001b[39m \u001b[38;5;66;03m# compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m scores = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m position_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    530\u001b[39m     key_length = key_states.shape[-\u001b[32m2\u001b[39m]\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 6.12 GiB. GPU 0 has a total capacity of 3.63 GiB of which 2.25 GiB is free. Including non-PyTorch memory, this process has 1.33 GiB memory in use. Of the allocated memory 1.19 GiB is allocated by PyTorch, and 82.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "import importlib\n",
    "import evaluation_utils2 as ev2\n",
    "\n",
    "importlib.reload(ev2)\n",
    "n = len(test_cases)\n",
    "scores = 0\n",
    "for case in test_cases[:1]:\n",
    "    print(type(case[\"retrieved_context\"]))\n",
    "    scores = scores + ev2.llm_as_a_judge(llm= llm_eval,query=case[\"query\"],retrieved_docs=case[\"retrieved_context\"],reference_answer=case[\"ground_truth\"])\n",
    "print(scores/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import evaluation_utils as ev\n",
    "\n",
    "importlib.reload(ev)\n",
    "results = ev.RAGA_Evaluator(test_cases=[{'query': 'What is the capital of France?',\n",
    "                                            'ground_truth': 'Paris',\n",
    "                                            'retrieved_context': 'France is a country in Europe, and its capital is Paris.',\n",
    "                                            'generated_answer': 'Paris'}],\n",
    "                            llm=llm_eval, embeddings=embedding_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "df1 = results.to_pandas()\n",
    "\n",
    "# Iterate through rows\n",
    "for _, row in df1.iterrows():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n🔹 Evaluation Results:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# response = retrieval_chain.invoke({\n",
    "#     \"context\": \"\\n\".join(label_ecg),\n",
    "#     \"input\": f\"Following are the ECG features :\\n{label_ecg[0]}\\n Can they be assocaiated with heart arrythmias. If yes, specify the proper disease name\"\n",
    "# })\n",
    "# print(response['input'])\n",
    "\n",
    "# print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# retriever = db.as_retriever(   search_type=\"mmr\",\n",
    "#     search_kwargs={'k': 5, 'fetch_k': 50})\n",
    "# retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# response = retrieval_chain.invoke({\n",
    "#     \"context\": \"\\n\".join(label_ecg),\n",
    "#     \"input\": f\"Following are the ECG features :\\n{label_ecg[0]}\\n Can they be assocaiated with heart arrythmias. If yes, specify the proper disease name\"\n",
    "# })\n",
    "# print(response['input'])\n",
    "\n",
    "# print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# retriever = db.as_retriever(    search_type=\"similarity_score_threshold\",\n",
    "#     search_kwargs={'score_threshold': 0.5})\n",
    "# retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# response = retrieval_chain.invoke({\n",
    "#     \"context\": \"\\n\".join(label_ecg),\n",
    "#     \"input\": f\"Following are the ECG features :\\n{label_ecg[0]}\\n Can they be assocaiated with heart arrythmias. If yes, specify the proper disease name\"\n",
    "# })\n",
    "# print(response['input'])\n",
    "\n",
    "# print(response['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
